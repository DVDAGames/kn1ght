{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!uv pip install tokenizers polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_GAME = \"<|g_start|>\"\n",
    "END_GAME = \"<|g_end|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_TOKENS = [START_GAME, END_GAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_csv(\"../.data/chess_games_2025-01-15.csv\", null_values=[\"None\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df.select(\"PGN\").sample(n=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_text = []\n",
    "\n",
    "for game in sample.iter_rows():\n",
    "    if game[0]:\n",
    "        training_text.append(START_GAME + game[0].strip() + END_GAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# ignore `1.`, ` 2.`, ` `, etc. and get the actual moves as separate entries\n",
    "chunk_pattern = re.compile(r\"\"\" ?\\d+\\.|\\. ?| ?[-\\w]+|[#+]|\\s+\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, Regex\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.normalizers import NFD\n",
    "from tokenizers.pre_tokenizers import (\n",
    "    Split,\n",
    "    ByteLevel,\n",
    "    Sequence,\n",
    "    WhitespaceSplit,\n",
    "    PreTokenizer,\n",
    ")\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
    "from tokenizers.processors import ByteLevel as ByteLevelProcessor\n",
    "from tokenizers.trainers import BpeTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(\n",
    "    BPE(unk_token=\"[UNK]\", fuse_unk=True, continuing_subword_prefix=\"\")\n",
    ")\n",
    "\n",
    "tokenizer.normalizer = NFD()\n",
    "\n",
    "tokenizer.pre_tokenizer = Sequence(\n",
    "    [\n",
    "        # WhitespaceSplit(),\n",
    "        Split(pattern=Regex(r\"\"\" ?\\d+\\.|\\. ?| ?[-\\w]+|[#+]\"\"\"), behavior=\"isolated\"),\n",
    "        # TODO: figure why this adds random Ä  characters\n",
    "        # everywhere when we just want to avoid spaces\n",
    "        # ByteLevel(add_prefix_space=False),\n",
    "    ]\n",
    ")\n",
    "\n",
    "tokenizer.post_processor = ByteLevelProcessor(trim_offsets=True)\n",
    "tokenizer.decoder = ByteLevelDecoder()\n",
    "\n",
    "trainer = BpeTrainer(vocab_size=3072, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train_from_iterator([training_text], trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.e4 c6 2.d4 d5 3.e5 Bf5 4.Nf3 e6 5.Be3 h6 6.c3 Nd7 7.Qb3 Qb6 8.Nbd2 Qxb3 9.axb3 c5 10.Bb5 cxd4 11.Nxd4 Bh7 12.O-O Ne7 13.f4 Nf5 14.Nxf5 Bxf5 15.Bxa7 Kd8 16.Bd4 Rc8 17.b4 Be7 18.Ra7 Rb8 19.Nb3 f6 20.Na5 fxe5 21.fxe5 Kc8 22.Bxd7+ Kxd7 23.Rxb7+ Rxb7 24.Nxb7 Kc6 25.Na5+ Kb5 26.h3 Bd8 27.Nb7 Kc6 28.Nd6 Bg6 29.b5+ Kc7 30.Ra1 Kb8 31.Ra7 Rf8 32.Rb7+ Ka8 33.Ra7+ Kb8 34.b6 Bh7 35.Rd7 Bd3 36.Rb7+ Ka8 37.Ra7+ Kb8 38.b7 Bh7 39.Ra8+ \n"
     ]
    }
   ],
   "source": [
    "sample = df.sample(1).select(\"PGN\").item()\n",
    "\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[45, 73, 211, 106, 69, 174, 107, 121, 336, 104, 95, 181, 105, 196, 207, 108, 71, 227, 109, 326, 296, 110, 357, 1064, 111, 721, 158, 112, 273, 219, 113, 231, 1485, 114, 67, 306, 115, 76, 454, 117, 821, 618, 120, 1195, 614, 122, 408, 241, 123, 133, 187, 124, 519, 259, 128, 362, 264, 129, 940, 620, 132, 423, 1066, 135, 767, 2, 1067, 136, 774, 2, 1120, 139, 1042, 500, 140, 940, 2, 1497, 143, 146, 580, 147, 1097, 500, 153, 505, 552, 156, 164, 2, 435, 162, 483, 706, 166, 519, 358, 171, 531, 2, 1499, 173, 519, 2, 706, 180, 167, 1485, 186, 541, 700, 189, 531, 2, 1499, 192, 519, 2, 706, 197, 177, 1485, 200, 639, 2, 0]\n",
      "['1.', 'e4', ' c6', ' 2.', 'd4', ' d5', ' 3.', 'e5', ' Bf5', ' 4.', 'Nf3', ' e6', ' 5.', 'Be3', ' h6', ' 6.', 'c3', ' Nd7', ' 7.', 'Qb3', ' Qb6', ' 8.', 'Nbd2', ' Qxb3', ' 9.', 'axb3', ' c5', ' 10.', 'Bb5', ' cxd4', ' 11.', 'Nxd4', ' Bh7', ' 12.', 'O-O', ' Ne7', ' 13.', 'f4', ' Nf5', ' 14.', 'Nxf5', ' Bxf5', ' 15.', 'Bxa7', ' Kd8', ' 16.', 'Bd4', ' Rc8', ' 17.', 'b4', ' Be7', ' 18.', 'Ra7', ' Rb8', ' 19.', 'Nb3', ' f6', ' 20.', 'Na5', ' fxe5', ' 21.', 'fxe5', ' Kc8', ' 22.', 'Bxd7', '+', ' Kxd7', ' 23.', 'Rxb7', '+', ' Rxb7', ' 24.', 'Nxb7', ' Kc6', ' 25.', 'Na5', '+', ' Kb5', ' 26.', 'h3', ' Bd8', ' 27.', 'Nb7', ' Kc6', ' 28.', 'Nd6', ' Bg6', ' 29.', 'b5', '+', ' Kc7', ' 30.', 'Ra1', ' Kb8', ' 31.', 'Ra7', ' Rf8', ' 32.', 'Rb7', '+', ' Ka8', ' 33.', 'Ra7', '+', ' Kb8', ' 34.', 'b6', ' Bh7', ' 35.', 'Rd7', ' Bd3', ' 36.', 'Rb7', '+', ' Ka8', ' 37.', 'Ra7', '+', ' Kb8', ' 38.', 'b7', ' Bh7', ' 39.', 'Ra8', '+', ' ']\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.encode(sample)\n",
    "\n",
    "print(output.ids)\n",
    "print(output.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding(num_tokens=126, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
      "1.e4 c6 2.d4 d5 3.e5 Bf5 4.Nf3 e6 5.Be3 h6 6.c3 Nd7 7.Qb3 Qb6 8.Nbd2 Qxb3 9.axb3 c5 10.Bb5 cxd4 11.Nxd4 Bh7 12.O-O Ne7 13.f4 Nf5 14.Nxf5 Bxf5 15.Bxa7 Kd8 16.Bd4 Rc8 17.b4 Be7 18.Ra7 Rb8 19.Nb3 f6 20.Na5 fxe5 21.fxe5 Kc8 22.Bxd7+ Kxd7 23.Rxb7+ Rxb7 24.Nxb7 Kc6 25.Na5+ Kb5 26.h3 Bd8 27.Nb7 Kc6 28.Nd6 Bg6 29.b5+ Kc7 30.Ra1 Kb8 31.Ra7 Rf8 32.Rb7+ Ka8 33.Ra7+ Kb8 34.b6 Bh7 35.Rd7 Bd3 36.Rb7+ Ka8 37.Ra7+ Kb8 38.b7 Bh7 39.Ra8+ \n",
      "True\n"
     ]
    }
   ],
   "source": [
    "test = tokenizer.decode(output.ids)\n",
    "print(output)\n",
    "print(test)\n",
    "print(test == sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.d4 d5 2.Nf3 Bf5'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(\"1.d4 d5 2.Nf3 Bf5\").ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
