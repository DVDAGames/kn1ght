{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!uv pip install tokenizers polars tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[g_start]', '[g_end]', '[unknown]']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kn1ght.constants import SPECIAL_TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_csv(\"../.data/chess_games_2025-01-15.csv\", null_values=[\"None\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df.select(\"PGN\").sample(n=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[g_start]1.d4 Nf6 2.c4 g6 3.Nc3 Bg7 4.e4 d6 5.Bd3 O-O 6.Nge2 e5 7.d5 Nbd7 8.Bg5 h6 9.Be3 a5 10.f3 Nc5 11.Bc2 Ne8 12.O-O f5 13.Bxc5 dxc5 14.exf5 gxf5 15.Ng3 Nd6 16.Qd3 Qh4 17.b3 Bd7 18.Rae1 Rae8 19.Nce2 Qg5 20.Kh1 h5 21.Ng1 h4 22.Nh3 Qh6 23.Ne2 Kh8 24.Nf2 Bf6 25.Ng1 Rg8 26.Nfh3 Rg7 27.Re2 Reg8 28.Qd2 f4 29.Qxa5 b6 30.Qd2 Bf5 31.Rff2 Qg6 32.a4 Qf7 33.Qe1 Qd7 34.Rf1 Bxc2 35.Rxc2 Nf5 36.Re2 Nd4 37.Rxe5 Kh7 38.Nxf4 Bxe5 39.Qxe5 Re8 40.Ne6 Nxe6 41.dxe6 Rxe6 42.Qb2 Qe7 43.Qc3 Kg8 44.Nh3 Re3 45.Qd2 Re2 46.Qd5+ Kh8 47.Nf4 Re5 48.Qc6 Qf7 49.Nd5 Re6[g_end]\n"
     ]
    }
   ],
   "source": [
    "training_text = []\n",
    "\n",
    "for game in sample.iter_rows():\n",
    "    if game[0]:\n",
    "        training_text.append(\n",
    "            SPECIAL_TOKENS[\"START\"] + game[0].strip() + SPECIAL_TOKENS[\"END\"]\n",
    "        )\n",
    "\n",
    "print(training_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# ignore `1.`, ` 2.`, ` `, etc. and get the actual moves as separate entries\n",
    "chunk_pattern = re.compile(r\"\"\" ?\\d+\\.|\\. ?| ?[-\\w]+|[#+]|\\s+\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, Regex\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.normalizers import NFD\n",
    "from tokenizers.pre_tokenizers import Split\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
    "from tokenizers.processors import ByteLevel as ByteLevelProcessor\n",
    "from tokenizers.trainers import BpeTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(BPE(unk_token=SPECIAL_TOKENS[\"UNKNOWN\"], fuse_unk=True))\n",
    "\n",
    "tokenizer.normalizer = NFD()\n",
    "\n",
    "tokenizer.pre_tokenizer = Split(\n",
    "    pattern=Regex(r\"\"\" ?\\d+\\.|\\. ?| ?[-\\w]+|[#+]|\\s+|\"\"\"), behavior=\"isolated\"\n",
    ")\n",
    "\n",
    "tokenizer.post_processor = ByteLevelProcessor(trim_offsets=True)\n",
    "tokenizer.decoder = ByteLevelDecoder()\n",
    "\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=4096, show_progress=True, special_tokens=list(SPECIAL_TOKENS.values())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train_from_iterator([training_text], trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.e4 c5 2.Nf3 Nc6 3.d4 cxd4 4.Nxd4 Nf6 5.Nc3 e5 6.Ndb5 d6 7.Bg5 a6 8.Na3 b5 9.Nd5 Qa5+ 10.Bd2 Qd8 11.Nxf6+ Qxf6 12.Bd3 Be7 13.O-O Qg6 14.f4 exf4 15.Bxf4 Ne5 16.Kh1 O-O 17.Qe2 Bf6 18.c3 Nxd3 19.Qxd3 Bb7 20.Qxd6 Qxe4 21.Qd2 Rad8 22.Qc2 Rfe8 23.Kg1 b4 24.cxb4 Bd4+ 25.Kh1 Qe2 26.Rg1 Bxb2 27.Qxe2 Rxe2 28.Rad1 Bxg2+ \n"
     ]
    }
   ],
   "source": [
    "sample = df.sample(1).select(\"PGN\").item()\n",
    "\n",
    "output = tokenizer.encode(sample)\n",
    "\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from: https://github.com/openai/tiktoken/blob/main/tiktoken/_educational.py#L186\n",
    "def visualise_kn1ght_tokens(token_values: list[str | bytes]) -> None:\n",
    "    background = [f\"\\u001b[48;5;{i}m\" for i in [167, 179, 185, 77, 80, 68, 134]]\n",
    "    # If token boundaries do not occur at unicode character boundaries, it's unclear how best to\n",
    "    # visualise the token. Here, we'll just use the unicode replacement character to represent some\n",
    "    # fraction of a character.\n",
    "    if not all(isinstance(x, str) for x in token_values):\n",
    "        unicode_token_values = [x.decode(\"utf-8\") for x in token_values]\n",
    "    else:\n",
    "        unicode_token_values = token_values\n",
    "\n",
    "    running_length = 0\n",
    "    last_color = None\n",
    "    for token in unicode_token_values:\n",
    "        color = background[running_length % len(background)]\n",
    "        if color == last_color:\n",
    "            color = background[(running_length + 1) % len(background)]\n",
    "            assert color != last_color\n",
    "        last_color = color\n",
    "        running_length += len(token)\n",
    "        print(color + token, end=\"\")\n",
    "    print(\"\\u001b[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tiktoken_gpt4_encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "tiktoken_gpt4_tokens = tiktoken_gpt4_encoding.encode(sample)\n",
    "\n",
    "tiktoken_gpt4o_encoding = tiktoken.get_encoding(\"o200k_base\")\n",
    "tiktoken_gpt4o_tokens = tiktoken_gpt4o_encoding.encode(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken gpt-4o Tokenizer:\n",
      "\u001b[48;5;167m1\u001b[48;5;179m.e\u001b[48;5;77m4\u001b[48;5;80m c\u001b[48;5;134m5\u001b[48;5;167m \u001b[48;5;179m2\u001b[48;5;185m.N\u001b[48;5;80mf\u001b[48;5;68m3\u001b[48;5;134m Nc\u001b[48;5;185m6\u001b[48;5;77m \u001b[48;5;80m3\u001b[48;5;68m.d\u001b[48;5;167m4\u001b[48;5;179m c\u001b[48;5;77mxd\u001b[48;5;68m4\u001b[48;5;134m \u001b[48;5;167m4\u001b[48;5;179m.N\u001b[48;5;77mxd\u001b[48;5;68m4\u001b[48;5;134m N\u001b[48;5;179mf\u001b[48;5;185m6\u001b[48;5;77m \u001b[48;5;80m5\u001b[48;5;68m.N\u001b[48;5;167mc\u001b[48;5;179m3\u001b[48;5;185m e\u001b[48;5;80m5\u001b[48;5;68m \u001b[48;5;134m6\u001b[48;5;167m.N\u001b[48;5;185mdb\u001b[48;5;80m5\u001b[48;5;68m d\u001b[48;5;167m6\u001b[48;5;179m \u001b[48;5;185m7\u001b[48;5;77m.B\u001b[48;5;68mg\u001b[48;5;134m5\u001b[48;5;167m a\u001b[48;5;185m6\u001b[48;5;77m \u001b[48;5;80m8\u001b[48;5;68m.Na\u001b[48;5;179m3\u001b[48;5;185m b\u001b[48;5;80m5\u001b[48;5;68m \u001b[48;5;134m9\u001b[48;5;167m.N\u001b[48;5;185md\u001b[48;5;77m5\u001b[48;5;80m Qa\u001b[48;5;167m5\u001b[48;5;179m+\u001b[48;5;185m \u001b[48;5;77m10\u001b[48;5;68m.B\u001b[48;5;167md\u001b[48;5;179m2\u001b[48;5;185m Q\u001b[48;5;80md\u001b[48;5;68m8\u001b[48;5;134m \u001b[48;5;167m11\u001b[48;5;185m.N\u001b[48;5;80mxf\u001b[48;5;134m6\u001b[48;5;167m+\u001b[48;5;179m Q\u001b[48;5;77mxf\u001b[48;5;68m6\u001b[48;5;134m \u001b[48;5;167m12\u001b[48;5;185m.B\u001b[48;5;80md\u001b[48;5;68m3\u001b[48;5;134m Be\u001b[48;5;185m7\u001b[48;5;77m \u001b[48;5;80m13\u001b[48;5;134m.O\u001b[48;5;179m-O\u001b[48;5;77m Q\u001b[48;5;68mg\u001b[48;5;134m6\u001b[48;5;167m \u001b[48;5;179m14\u001b[48;5;77m.f\u001b[48;5;68m4\u001b[48;5;134m ex\u001b[48;5;185mf\u001b[48;5;77m4\u001b[48;5;80m \u001b[48;5;68m15\u001b[48;5;167m.B\u001b[48;5;185mxf\u001b[48;5;80m4\u001b[48;5;68m Ne\u001b[48;5;179m5\u001b[48;5;185m \u001b[48;5;77m16\u001b[48;5;68m.K\u001b[48;5;167mh\u001b[48;5;179m1\u001b[48;5;185m O\u001b[48;5;80m-O\u001b[48;5;134m \u001b[48;5;167m17\u001b[48;5;185m.Q\u001b[48;5;80me\u001b[48;5;68m2\u001b[48;5;134m B\u001b[48;5;179mf\u001b[48;5;185m6\u001b[48;5;77m \u001b[48;5;80m18\u001b[48;5;134m.c\u001b[48;5;179m3\u001b[48;5;185m N\u001b[48;5;80mxd\u001b[48;5;134m3\u001b[48;5;167m \u001b[48;5;179m19\u001b[48;5;77m.Q\u001b[48;5;68mxd\u001b[48;5;167m3\u001b[48;5;179m Bb\u001b[48;5;80m7\u001b[48;5;68m \u001b[48;5;134m20\u001b[48;5;179m.Q\u001b[48;5;77mxd\u001b[48;5;68m6\u001b[48;5;134m Q\u001b[48;5;179mxe\u001b[48;5;77m4\u001b[48;5;80m \u001b[48;5;68m21\u001b[48;5;167m.Q\u001b[48;5;185md\u001b[48;5;77m2\u001b[48;5;80m Rad\u001b[48;5;179m8\u001b[48;5;185m \u001b[48;5;77m22\u001b[48;5;68m.Q\u001b[48;5;167mc\u001b[48;5;179m2\u001b[48;5;185m R\u001b[48;5;80mfe\u001b[48;5;134m8\u001b[48;5;167m \u001b[48;5;179m23\u001b[48;5;77m.K\u001b[48;5;68mg\u001b[48;5;134m1\u001b[48;5;167m b\u001b[48;5;185m4\u001b[48;5;77m \u001b[48;5;80m24\u001b[48;5;134m.c\u001b[48;5;179mxb\u001b[48;5;77m4\u001b[48;5;80m Bd\u001b[48;5;167m4\u001b[48;5;179m+\u001b[48;5;185m \u001b[48;5;77m25\u001b[48;5;68m.K\u001b[48;5;167mh\u001b[48;5;179m1\u001b[48;5;185m Q\u001b[48;5;80me\u001b[48;5;68m2\u001b[48;5;134m \u001b[48;5;167m26\u001b[48;5;185m.R\u001b[48;5;80mg\u001b[48;5;68m1\u001b[48;5;134m B\u001b[48;5;179mxb\u001b[48;5;77m2\u001b[48;5;80m \u001b[48;5;68m27\u001b[48;5;167m.Q\u001b[48;5;185mxe\u001b[48;5;80m2\u001b[48;5;68m R\u001b[48;5;167mxe\u001b[48;5;185m2\u001b[48;5;77m \u001b[48;5;80m28\u001b[48;5;134m.Rad\u001b[48;5;77m1\u001b[48;5;80m B\u001b[48;5;134mx\u001b[48;5;167mg\u001b[48;5;179m2\u001b[48;5;185m+\u001b[48;5;77m \u001b[0m\n",
      "---\n",
      "tiktoken gpt-4 Tokenizer:\n",
      "\u001b[48;5;167m1\u001b[48;5;179m.e\u001b[48;5;77m4\u001b[48;5;80m c\u001b[48;5;134m5\u001b[48;5;167m \u001b[48;5;179m2\u001b[48;5;185m.N\u001b[48;5;80mf\u001b[48;5;68m3\u001b[48;5;134m N\u001b[48;5;179mc\u001b[48;5;185m6\u001b[48;5;77m \u001b[48;5;80m3\u001b[48;5;68m.d\u001b[48;5;167m4\u001b[48;5;179m c\u001b[48;5;77mxd\u001b[48;5;68m4\u001b[48;5;134m \u001b[48;5;167m4\u001b[48;5;179m.N\u001b[48;5;77mxd\u001b[48;5;68m4\u001b[48;5;134m N\u001b[48;5;179mf\u001b[48;5;185m6\u001b[48;5;77m \u001b[48;5;80m5\u001b[48;5;68m.N\u001b[48;5;167mc\u001b[48;5;179m3\u001b[48;5;185m e\u001b[48;5;80m5\u001b[48;5;68m \u001b[48;5;134m6\u001b[48;5;167m.N\u001b[48;5;185mdb\u001b[48;5;80m5\u001b[48;5;68m d\u001b[48;5;167m6\u001b[48;5;179m \u001b[48;5;185m7\u001b[48;5;77m.B\u001b[48;5;68mg\u001b[48;5;134m5\u001b[48;5;167m a\u001b[48;5;185m6\u001b[48;5;77m \u001b[48;5;80m8\u001b[48;5;68m.N\u001b[48;5;167ma\u001b[48;5;179m3\u001b[48;5;185m b\u001b[48;5;80m5\u001b[48;5;68m \u001b[48;5;134m9\u001b[48;5;167m.N\u001b[48;5;185md\u001b[48;5;77m5\u001b[48;5;80m Q\u001b[48;5;134ma\u001b[48;5;167m5\u001b[48;5;179m+\u001b[48;5;185m \u001b[48;5;77m10\u001b[48;5;68m.B\u001b[48;5;167md\u001b[48;5;179m2\u001b[48;5;185m Q\u001b[48;5;80md\u001b[48;5;68m8\u001b[48;5;134m \u001b[48;5;167m11\u001b[48;5;185m.N\u001b[48;5;80mxf\u001b[48;5;134m6\u001b[48;5;167m+\u001b[48;5;179m Q\u001b[48;5;77mxf\u001b[48;5;68m6\u001b[48;5;134m \u001b[48;5;167m12\u001b[48;5;185m.B\u001b[48;5;80md\u001b[48;5;68m3\u001b[48;5;134m Be\u001b[48;5;185m7\u001b[48;5;77m \u001b[48;5;80m13\u001b[48;5;134m.O\u001b[48;5;179m-O\u001b[48;5;77m Q\u001b[48;5;68mg\u001b[48;5;134m6\u001b[48;5;167m \u001b[48;5;179m14\u001b[48;5;77m.f\u001b[48;5;68m4\u001b[48;5;134m ex\u001b[48;5;185mf\u001b[48;5;77m4\u001b[48;5;80m \u001b[48;5;68m15\u001b[48;5;167m.B\u001b[48;5;185mxf\u001b[48;5;80m4\u001b[48;5;68m Ne\u001b[48;5;179m5\u001b[48;5;185m \u001b[48;5;77m16\u001b[48;5;68m.K\u001b[48;5;167mh\u001b[48;5;179m1\u001b[48;5;185m O\u001b[48;5;80m-O\u001b[48;5;134m \u001b[48;5;167m17\u001b[48;5;185m.Q\u001b[48;5;80me\u001b[48;5;68m2\u001b[48;5;134m B\u001b[48;5;179mf\u001b[48;5;185m6\u001b[48;5;77m \u001b[48;5;80m18\u001b[48;5;134m.c\u001b[48;5;179m3\u001b[48;5;185m N\u001b[48;5;80mxd\u001b[48;5;134m3\u001b[48;5;167m \u001b[48;5;179m19\u001b[48;5;77m.Q\u001b[48;5;68mxd\u001b[48;5;167m3\u001b[48;5;179m B\u001b[48;5;77mb\u001b[48;5;80m7\u001b[48;5;68m \u001b[48;5;134m20\u001b[48;5;179m.Q\u001b[48;5;77mxd\u001b[48;5;68m6\u001b[48;5;134m Q\u001b[48;5;179mxe\u001b[48;5;77m4\u001b[48;5;80m \u001b[48;5;68m21\u001b[48;5;167m.Q\u001b[48;5;185md\u001b[48;5;77m2\u001b[48;5;80m Rad\u001b[48;5;179m8\u001b[48;5;185m \u001b[48;5;77m22\u001b[48;5;68m.Q\u001b[48;5;167mc\u001b[48;5;179m2\u001b[48;5;185m R\u001b[48;5;80mfe\u001b[48;5;134m8\u001b[48;5;167m \u001b[48;5;179m23\u001b[48;5;77m.K\u001b[48;5;68mg\u001b[48;5;134m1\u001b[48;5;167m b\u001b[48;5;185m4\u001b[48;5;77m \u001b[48;5;80m24\u001b[48;5;134m.c\u001b[48;5;179mxb\u001b[48;5;77m4\u001b[48;5;80m Bd\u001b[48;5;167m4\u001b[48;5;179m+\u001b[48;5;185m \u001b[48;5;77m25\u001b[48;5;68m.K\u001b[48;5;167mh\u001b[48;5;179m1\u001b[48;5;185m Q\u001b[48;5;80me\u001b[48;5;68m2\u001b[48;5;134m \u001b[48;5;167m26\u001b[48;5;185m.R\u001b[48;5;80mg\u001b[48;5;68m1\u001b[48;5;134m B\u001b[48;5;179mxb\u001b[48;5;77m2\u001b[48;5;80m \u001b[48;5;68m27\u001b[48;5;167m.Q\u001b[48;5;185mxe\u001b[48;5;80m2\u001b[48;5;68m R\u001b[48;5;167mxe\u001b[48;5;185m2\u001b[48;5;77m \u001b[48;5;80m28\u001b[48;5;134m.Rad\u001b[48;5;77m1\u001b[48;5;80m B\u001b[48;5;134mx\u001b[48;5;167mg\u001b[48;5;179m2\u001b[48;5;185m+\u001b[48;5;77m \u001b[0m\n",
      "---\n",
      "kn1ght Tokenizer:\n",
      "\u001b[48;5;167m1.\u001b[48;5;185me4\u001b[48;5;80m c5\u001b[48;5;167m 2.\u001b[48;5;77mNf3\u001b[48;5;134m Nc6\u001b[48;5;77m 3.\u001b[48;5;134md4\u001b[48;5;179m cxd4\u001b[48;5;134m 4.\u001b[48;5;185mNxd4\u001b[48;5;134m Nf6\u001b[48;5;77m 5.\u001b[48;5;134mNc3\u001b[48;5;185m e5\u001b[48;5;68m 6.\u001b[48;5;179mNdb5\u001b[48;5;68m d6\u001b[48;5;179m 7.\u001b[48;5;80mBg5\u001b[48;5;167m a6\u001b[48;5;77m 8.\u001b[48;5;134mNa3\u001b[48;5;185m b5\u001b[48;5;68m 9.\u001b[48;5;179mNd5\u001b[48;5;80m Qa5\u001b[48;5;179m+\u001b[48;5;185m 10.\u001b[48;5;134mBd2\u001b[48;5;185m Qd8\u001b[48;5;134m 11.\u001b[48;5;77mNxf6\u001b[48;5;167m+\u001b[48;5;179m Qxf6\u001b[48;5;134m 12.\u001b[48;5;77mBd3\u001b[48;5;134m Be7\u001b[48;5;77m 13.\u001b[48;5;167mO-O\u001b[48;5;77m Qg6\u001b[48;5;167m 14.\u001b[48;5;80mf4\u001b[48;5;134m exf4\u001b[48;5;80m 15.\u001b[48;5;179mBxf4\u001b[48;5;68m Ne5\u001b[48;5;185m 16.\u001b[48;5;134mKh1\u001b[48;5;185m O-O\u001b[48;5;134m 17.\u001b[48;5;77mQe2\u001b[48;5;134m Bf6\u001b[48;5;77m 18.\u001b[48;5;167mc3\u001b[48;5;185m Nxd3\u001b[48;5;167m 19.\u001b[48;5;80mQxd3\u001b[48;5;179m Bb7\u001b[48;5;68m 20.\u001b[48;5;185mQxd6\u001b[48;5;134m Qxe4\u001b[48;5;80m 21.\u001b[48;5;179mQd2\u001b[48;5;80m Rad8\u001b[48;5;185m 22.\u001b[48;5;134mQc2\u001b[48;5;185m Rfe8\u001b[48;5;167m 23.\u001b[48;5;80mKg1\u001b[48;5;167m b4\u001b[48;5;77m 24.\u001b[48;5;167mcxb4\u001b[48;5;80m Bd4\u001b[48;5;179m+\u001b[48;5;185m 25.\u001b[48;5;134mKh1\u001b[48;5;185m Qe2\u001b[48;5;134m 26.\u001b[48;5;77mRg1\u001b[48;5;134m Bxb2\u001b[48;5;80m 27.\u001b[48;5;179mQxe2\u001b[48;5;68m Rxe2\u001b[48;5;77m 28.\u001b[48;5;167mRad1\u001b[48;5;80m Bxg2\u001b[48;5;185m+\u001b[48;5;77m \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(\"tiktoken gpt-4o Tokenizer:\")\n",
    "visualise_kn1ght_tokens(\n",
    "    tiktoken_gpt4o_encoding.decode_tokens_bytes(tiktoken_gpt4o_tokens)\n",
    ")\n",
    "print(\"---\")\n",
    "print(\"tiktoken gpt-4 Tokenizer:\")\n",
    "visualise_kn1ght_tokens(\n",
    "    tiktoken_gpt4_encoding.decode_tokens_bytes(tiktoken_gpt4_tokens)\n",
    ")\n",
    "print(\"---\")\n",
    "print(\"kn1ght Tokenizer:\")\n",
    "visualise_kn1ght_tokens(output.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding(num_tokens=89, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
      "1.e4 c5 2.Nf3 Nc6 3.d4 cxd4 4.Nxd4 Nf6 5.Nc3 e5 6.Ndb5 d6 7.Bg5 a6 8.Na3 b5 9.Nd5 Qa5+ 10.Bd2 Qd8 11.Nxf6+ Qxf6 12.Bd3 Be7 13.O-O Qg6 14.f4 exf4 15.Bxf4 Ne5 16.Kh1 O-O 17.Qe2 Bf6 18.c3 Nxd3 19.Qxd3 Bb7 20.Qxd6 Qxe4 21.Qd2 Rad8 22.Qc2 Rfe8 23.Kg1 b4 24.cxb4 Bd4+ 25.Kh1 Qe2 26.Rg1 Bxb2 27.Qxe2 Rxe2 28.Rad1 Bxg2+ \n",
      "True\n"
     ]
    }
   ],
   "source": [
    "test = tokenizer.decode(output.ids)\n",
    "print(output)\n",
    "print(test)\n",
    "print(test == sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.d4 d5 2.Nf3 Bf5'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(\"1.d4 d5 2.Nf3 Bf5\").ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.model.save(\"../.data/tokenizer/\", \"kn1ght\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[g_start]']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(START_GAME).tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
