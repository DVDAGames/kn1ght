{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!uv pip install torch tokenizers numpy datasets transformers[torch] accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    PreTrainedTokenizerFast,\n",
    "    GPT2Config,\n",
    "    GPT2LMHeadModel,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "mps_device = None\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print(x)\n",
    "else:\n",
    "    print(\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"../.data/datasets/full/2025-01-20\"\n",
    "TOKENIZER_PATH = \"../src/tokenizer/kn1ght-tokenizer.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Date', 'Result', 'WhiteElo', 'BlackElo', 'PGN'],\n",
      "        num_rows: 352349\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['Date', 'Result', 'WhiteElo', 'BlackElo', 'PGN'],\n",
      "        num_rows: 1585571\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['Date', 'Result', 'WhiteElo', 'BlackElo', 'PGN'],\n",
      "        num_rows: 1585572\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "raw_dataset = load_from_disk(DATASET_PATH)\n",
    "\n",
    "print(raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kn1ght.constants import SPECIAL_TOKENS\n",
    "\n",
    "tokenizer.pad_token = SPECIAL_TOKENS[\"PAD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device = mps_device if mps_device is not None else device\n",
    "\n",
    "block_size = 512\n",
    "batch_size = 1000\n",
    "num_proc = 4\n",
    "mlm = False\n",
    "n_embed = 768\n",
    "n_layer = 12\n",
    "n_head = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data(dataset, samples=1000) -> dict:\n",
    "    return dataset.shuffle(seed=1997).select(range(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(examples):\n",
    "    return tokenizer(examples[\"PGN\"], return_special_tokens_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = sample_data(raw_dataset[\"train\"], 11000)\n",
    "\n",
    "tokenized_datasets = training_data.map(\n",
    "    tokenize,\n",
    "    batched=True,\n",
    "    num_proc=num_proc,\n",
    "    remove_columns=[\"Date\", \"Result\", \"WhiteElo\", \"BlackElo\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['PGN', 'input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
      "    num_rows: 11000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=mlm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPT2Config(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    n_embed=n_embed,\n",
    "    n_layer=n_layer,\n",
    "    n_head=n_head,\n",
    "    bos_token_id=0,\n",
    "    eos_token_id=1,\n",
    "    unk_token_id=2,\n",
    "    pad_token_id=3,\n",
    ")\n",
    "\n",
    "model = GPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../.data/models/kn1ght/2025-01-20\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=1,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=2000,\n",
    "    save_steps=2000,\n",
    "    logging_steps=500,\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=1000,\n",
    "    report_to=\"none\",\n",
    "    seed=1997,\n",
    "    fp16=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['PGN', 'input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
      "    num_rows: 10000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['PGN', 'input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
      "    num_rows: 1000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = tokenized_datasets.train_test_split(test_size=1000)\n",
    "\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"] if \"test\" in dataset else None\n",
    "\n",
    "print(train_dataset)\n",
    "print(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 89.0M parameters\n"
     ]
    }
   ],
   "source": [
    "model_size = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"Model size: {model_size / 1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': '[g_start]1.d4 d5 2.Nf3 141.'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "from kn1ght.constants import SPECIAL_TOKENS\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=mps_device)\n",
    "\n",
    "prompt = f\"{SPECIAL_TOKENS['START']}1.d4 d5 2.Nf3\"\n",
    "\n",
    "output = pipe(\n",
    "    prompt,\n",
    "    max_length=len(tokenizer.encode(prompt)) + 1,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    top_k=50,\n",
    "    truncation=True,\n",
    ")\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': '[g_start]1.e4 c5 2.Nf3 Nc6 3.d4 cxd4 4.Nxd4 Nf6 5.Nc3 e5 6.Ndb5 d6 7.Bg5 a6 8.Na3 b5 9.Nd5 Qa5+ 10.Bd2 Qd8 11.Nxf6+ Qxf6 12.Bd3 Be7 13.O-O Qg6 14.f4 exf4 15.Bxf4 Ne5 16.Kh1 O-O 17.Qe2 Rad2'}]\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"{SPECIAL_TOKENS['START']}1.e4 c5 2.Nf3 Nc6 3.d4 cxd4 4.Nxd4 Nf6 5.Nc3 e5 6.Ndb5 d6 7.Bg5 a6 8.Na3 b5 9.Nd5 Qa5+ 10.Bd2 Qd8 11.Nxf6+ Qxf6 12.Bd3 Be7 13.O-O Qg6 14.f4 exf4 15.Bxf4 Ne5 16.Kh1 O-O 17.Qe2\"\n",
    "\n",
    "output = pipe(\n",
    "    prompt,\n",
    "    max_length=len(tokenizer.encode(prompt)) + 1,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    top_k=50,\n",
    "    truncation=True,\n",
    ")\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"../.data/models/out/kn1ght\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(4096, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=4096, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.from_pretrained(\"../.data/models/out/kn1ght\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f6aa1eeb3ec42e793e795a66a096ff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/356M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/InterwebAlchemy/kn1ght/commit/345038e7c382c79dde8c319180735d48029fcf05', commit_message='Upload model', commit_description='', oid='345038e7c382c79dde8c319180735d48029fcf05', pr_url=None, repo_url=RepoUrl('https://huggingface.co/InterwebAlchemy/kn1ght', endpoint='https://huggingface.co', repo_type='model', repo_id='InterwebAlchemy/kn1ght'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"InterwebAlchemy/kn1ght\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
